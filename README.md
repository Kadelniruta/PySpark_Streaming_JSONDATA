ğŸ“Š Complex JSON Flattening in Databricks using PySpark
ğŸ“Œ Overview

This project demonstrates how to read, process, and flatten deeply nested JSON data using PySpark in Databricks.
The source JSON represents a company structure with multiple nested levels including departments, teams, members, and projects.

The goal is to transform hierarchical JSON data into a flat, analytics-ready tabular format suitable for reporting, SQL analytics, and downstream processing.

ğŸ› ï¸ Technology Stack

Platform: Databricks

Language: PySpark

Data Format: JSON

Processing Type: Batch Processing

Libraries Used:

pyspark.sql.functions

pyspark.sql.types

ğŸ“‚ Input Data Description

The input JSON contains the following nested structure:

company
 â”œâ”€â”€ name
 â”œâ”€â”€ founded
 â”œâ”€â”€ headquarters
 â”‚    â”œâ”€â”€ address
 â”‚    â””â”€â”€ coordinates
 â”œâ”€â”€ departments (ARRAY)
 â”‚    â”œâ”€â”€ id
 â”‚    â”œâ”€â”€ name
 â”‚    â”œâ”€â”€ budget
 â”‚    â””â”€â”€ teams (ARRAY)
 â”‚         â”œâ”€â”€ teamId
 â”‚         â”œâ”€â”€ lead
 â”‚         â””â”€â”€ members (ARRAY)
 â”‚              â”œâ”€â”€ employeeId
 â”‚              â”œâ”€â”€ skills (ARRAY)
 â”‚              â””â”€â”€ projects (ARRAY)
 â””â”€â”€ metrics
      â”œâ”€â”€ employees
      â””â”€â”€ revenue

ğŸ“¥ Reading the JSON Data

The JSON file is read in batch mode with schema inference enabled.

df = spark.read.format('json') \
    .option("inferschema", True) \
    .option("multiline", True) \
    .load("/Volumes/workspace/pysparkcsv/companycomplex")


multiline = true allows reading formatted JSON

inferschema = true automatically detects nested structures

ğŸ”„ Transformation Logic
1ï¸âƒ£ Exploding Nested Arrays

To flatten the hierarchical structure, multiple explode_outer() operations are used:

df = df.withColumn("departments", explode_outer("company.departments")) \
       .withColumn("teams", explode_outer("departments.teams")) \
       .withColumn("members", explode_outer("teams.members")) \
       .withColumn("projects", explode_outer("members.projects"))


This results in:

One row per company â†’ department â†’ team â†’ member â†’ project

Preserves rows even if nested arrays are empty (explode_outer)

2ï¸âƒ£ Selecting and Renaming Fields

Nested fields are extracted using dot notation and renamed for clarity:

.select(
    col("company.name").alias("companyName"),
    col("company.founded").alias("foundedYear"),
    col("company.headquarters.address.city").alias("hqCity"),
    col("departments.id").alias("departmentId"),
    col("teams.teamId").alias("teamId"),
    col("members.employeeId").alias("employeeId"),
    col("projects.projectId").alias("projectId")
)

3ï¸âƒ£ Handling Arrays Inside Fields

Employee skills (ARRAY) are converted into a readable string:

concat_ws(", ", col("members.skills")).alias("employeeSkills")

ğŸ“¤ Output Data

The final output is a fully flattened DataFrame containing:

Company-level attributes

Headquarters location details

Department budget information

Team and team lead details

Employee and skill information

Project allocation details

Company-wide employee and revenue metrics

This structure is ideal for:

BI tools

SQL analytics

Delta Lake storage

Reporting dashboards

âš ï¸ Important Notes

Multiple explode_outer() operations can lead to row multiplication

For large datasets, consider:

Storing raw JSON in a Bronze layer

Partial flattening in Silver

Full flattening only in Gold

ğŸ—ï¸ Best Practice Architecture
Raw JSON â†’ Bronze (Nested) â†’ Silver (Partial Flatten) â†’ Gold (Fully Flattened)

âœ… Conclusion

This Databricks notebook demonstrates:

Efficient handling of deeply nested JSON

Safe flattening using explode_outer

Transformation of complex structures into analytics-ready tables

The approach follows industry best practices for large-scale data processing using PySpark.
