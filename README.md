# Complex JSON Processing & IoT Sensor Streaming using PySpark

## ğŸ“Œ Overview

This repository contains **two real-world PySpark data engineering implementations** executed in a **Databricks environment**:

1. **Company Complex JSON Processing** â€“ Batch-style processing of deeply nested enterprise company data
2. **IoT Sensor Streaming JSON Processing** â€“ Near real-time style processing of hierarchical IoT sensor data

Both pipelines demonstrate **best practices for handling complex nested JSON**, schema normalization, and building **analytics-ready flat tables** using Apache Spark.

---

## ğŸ—ï¸ Overall Architecture

```
Source JSON (Nested)
      â†“
Spark Read (JSON, inferSchema, multiline)
      â†“
explode_outer (Hierarchical Arrays)
      â†“
Column Selection & Aliasing
      â†“
Flattened DataFrames
      â†“
Analytics / BI / Data Warehouse
```

---

## ğŸ§° Technologies Used

* Apache Spark (PySpark)
* Databricks Notebooks
* Spark SQL Functions
* Databricks Volumes

---

# ğŸ“˜ Module 1: Company Complex JSON Processing

## ğŸ¯ Objective

To transform a **deeply nested company JSON structure** containing departments, teams, employees, projects, and metrics into a **single flattened DataFrame** suitable for reporting and analytics.

---

## ğŸ“‚ Data Source (Company)

* **Format:** JSON
* **Nature:** Batch / Historical
* **Location:**

```
/Volumes/workspace/pysparkcsv/companycomplex
```

---

## ğŸ§© JSON Structure (Logical)

```
company
 â”œâ”€â”€ headquarters
 â”œâ”€â”€ departments[]
 â”‚     â”œâ”€â”€ teams[]
 â”‚     â”‚     â”œâ”€â”€ members[]
 â”‚     â”‚     â”‚     â””â”€â”€ projects[]
 â””â”€â”€ metrics
       â”œâ”€â”€ employees
       â””â”€â”€ revenue
```

---

## ğŸ”„ Processing Steps

### 1ï¸âƒ£ Read Complex JSON

```python
df = spark.read.format('json') \
    .option("inferschema", True) \
    .option("multiline", True) \
    .load("/Volumes/workspace/pysparkcsv/companycomplex")
```

---

### 2ï¸âƒ£ Flatten Nested Arrays

Arrays are expanded using **`explode_outer`** to ensure no data loss when arrays are empty or null.

| Level      | Path                |
| ---------- | ------------------- |
| Department | company.departments |
| Team       | departments.teams   |
| Member     | teams.members       |
| Project    | members.projects    |

---

### 3ï¸âƒ£ Data Normalization

The pipeline selects and aliases fields into logical groups:

#### ğŸ”¹ Company & Headquarters

* Company Name, Founded Year
* Address & Geo Coordinates

#### ğŸ”¹ Department

* ID, Name
* Annual Budget
* Budget Breakdown

#### ğŸ”¹ Team

* Team ID, Name
* Team Lead Details

#### ğŸ”¹ Employee

* Employee ID, Name, Role
* Skills (comma-separated)

#### ğŸ”¹ Project

* Project ID, Name
* Allocation

#### ğŸ”¹ Company Metrics

* Employee distribution (Region & Type)
* Quarterly revenue (2024â€“2025)

---

## ğŸ“Š Output Characteristics (Company)

* Grain: **One row per employee-project-team-department**
* Fully denormalized
* BI-ready & warehouse-friendly

---

# ğŸ“— Module 2: IoT Sensor Streaming JSON Processing

## ğŸ¯ Objective

To process **IoT sensor JSON data** containing facilities, sensors, readings, alerts, and thresholds, converting it into a **time-series friendly flattened dataset**.

---

## ğŸ“‚ Data Source (IoT)

* **Format:** JSON
* **Nature:** Streaming-style / Incremental
* **Location:**

```
/Volumes/workspace/pysparkcsv/iot_sensor
```

---

## ğŸ§© JSON Structure (Logical)

```
facility
 â”œâ”€â”€ location
 â”œâ”€â”€ sensors[]
 â”‚     â”œâ”€â”€ manufacturer
 â”‚     â”œâ”€â”€ thresholds
 â”‚     â””â”€â”€ readings[]
 â”‚            â””â”€â”€ metadata
 â”‚                 â””â”€â”€ alerts[]
```

---

## ğŸ”„ Processing Steps

### 1ï¸âƒ£ Read IoT JSON

```python
df = spark.read.format('json') \
    .option("inferschema", True) \
    .option("multiline", True) \
    .load("/Volumes/workspace/pysparkcsv/iot_sensor")
```

---

### 2ï¸âƒ£ Hierarchical Explosion

Nested arrays are flattened in sequence:

| Level   | Path                     |
| ------- | ------------------------ |
| Sensor  | facility.sensors         |
| Reading | sensors.readings         |
| Alert   | readings.metadata.alerts |

---

### 3ï¸âƒ£ Data Normalization

#### ğŸ”¹ Facility Information

* Facility ID & Name
* Building, Floor, Zone

#### ğŸ”¹ Sensor Information

* Sensor ID & Type
* Manufacturer & Model
* Calibration Details
* Technician Info

#### ğŸ”¹ Readings Information

* Timestamp
* Value & Unit
* Quality & Confidence

#### ğŸ”¹ Alerts Information

* Alert ID
* Severity
* Message

#### ğŸ”¹ Thresholds

* Min / Max
* Critical Min / Max

---

## ğŸš¨ Alert Handling (IoT Module)

Alerts are generated when sensor readings **violate defined thresholds** or when abnormal behavior is detected by the sensor metadata.

### ğŸ”¹ Alert Source

Alerts are nested inside the following JSON path:

```
readings.metadata.alerts[]
```

Each alert is associated with a **specific sensor reading**, ensuring accurate traceability.

---

### ğŸ”¹ Alert Attributes

The following alert-related fields are extracted and flattened:

| Column           | Description                                     |
| ---------------- | ----------------------------------------------- |
| `alert_id`       | Unique identifier for the alert                 |
| `alert_severity` | Severity level (Low / Medium / High / Critical) |
| `alert_message`  | Human-readable description of the issue         |

---

### ğŸ”¹ Why Alert Processing Is Important

* Enables **real-time monitoring & incident response**
* Helps identify **sensor failures or abnormal conditions**
* Supports **SLA tracking and compliance**
* Critical for **predictive maintenance and safety systems**

---

### ğŸ”¹ Data Grain Impact

Including alerts changes the output grain to:

> **One row per sensor â†’ per reading â†’ per alert**

If a reading has multiple alerts, multiple rows are generated. If no alerts exist, `explode_outer` ensures the record is still retained.

---

## ğŸ“Š Output Characteristics (IoT)

* Grain: **One row per sensor-reading-alert**
* Time-series optimized
* Suitable for monitoring dashboards, anomaly detection & alerting systems

---

## âœ… Key Design Principles (Both Pipelines)

* `explode_outer` to prevent data loss
* Explicit column aliasing for clarity
* Flat schema for analytics efficiency
* Modular & reusable transformations

---

## âš ï¸ Assumptions

* JSON schema remains consistent
* Databricks runtime configured correctly
* Data volume fits Spark cluster capacity

---

## ğŸš€ Future Enhancements

* Convert batch to Structured Streaming
* Write output to Delta Lake
* Add schema enforcement
* Implement data quality checks
* Partition by date / facility / department

---

## ğŸ‘¤ Author

**PySpark Data Engineering Project**
Designed to demonstrate enterprise-grade handling of complex JSON and IoT data using Apache Spark.

---

## ğŸ Conclusion

This repository showcases **realistic enterprise and IoT data engineering use cases**, highlighting how PySpark can efficiently transform complex hierarchical JSON into actionable, analytics-ready datasets.
